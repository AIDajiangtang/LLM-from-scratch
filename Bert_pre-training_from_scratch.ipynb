{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5Oha7bVFJLl",
        "outputId": "bf63cb7f-61a7-4f67-f5c4-c574652b0e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch_bert'...\n",
            "remote: Enumerating objects: 150, done.\u001b[K\n",
            "remote: Counting objects: 100% (150/150), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 150 (delta 81), reused 141 (delta 74), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (150/150), 29.05 MiB | 7.34 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/coaxsoft/pytorch_bert.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pytorch_bert/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNmu9HdbKFTV",
        "outputId": "549e8691-36e4-413a-ed4e-879142559ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch_bert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9WT9Gn2Kdqx",
        "outputId": "302a8b12-fae6-4b0b-8b5a-e5066d65db67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.0.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.18.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2.15.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 2)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->-r requirements.txt (line 3))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext->-r requirements.txt (line 4)) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext->-r requirements.txt (line 4)) (2.31.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext->-r requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext->-r requirements.txt (line 4)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext->-r requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext->-r requirements.txt (line 4)) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 5)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 5)) (3.2.2)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import typing\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class IMDBBertDataset(Dataset):\n",
        "    CLS = '[CLS]'\n",
        "    PAD = '[PAD]'\n",
        "    SEP = '[SEP]'\n",
        "    MASK = '[MASK]'\n",
        "    UNK = '[UNK]'\n",
        "\n",
        "    MASK_PERCENTAGE = 0.15\n",
        "\n",
        "    MASKED_INDICES_COLUMN = 'masked_indices'\n",
        "    TARGET_COLUMN = 'indices'\n",
        "    NSP_TARGET_COLUMN = 'is_next'\n",
        "    TOKEN_MASK_COLUMN = 'token_mask'\n",
        "\n",
        "    OPTIMAL_LENGTH_PERCENTILE = 70\n",
        "\n",
        "    def __init__(self, path, ds_from=None, ds_to=None, should_include_text=False):\n",
        "        # 读取CSV文件，并将'review'列赋值给self.ds\n",
        "        self.ds: pd.Series = pd.read_csv(path)['review']\n",
        "\n",
        "        # 如果指定了ds_from或ds_to，则对self.ds进行切片操作\n",
        "        if ds_from is not None or ds_to is not None:\n",
        "            self.ds = self.ds[ds_from:ds_to]\n",
        "\n",
        "        # 获取基本英文分词器，并赋值给self.tokenizer\n",
        "        self.tokenizer = get_tokenizer('basic_english')\n",
        "        # 初始化计数器self.counter\n",
        "        self.counter = Counter()\n",
        "        # 初始化词汇表self.vocab，初始值为None\n",
        "        self.vocab = None\n",
        "\n",
        "        # 初始化最优句子长度self.optimal_sentence_length，初始值为None\n",
        "        self.optimal_sentence_length = None\n",
        "        # 初始化是否包含文本的标志self.should_include_text，并赋值\n",
        "        self.should_include_text = should_include_text\n",
        "\n",
        "        # 如果should_include_text为True，则设置self.columns为包含文本的列名列表\n",
        "        if should_include_text:\n",
        "            self.columns = ['masked_sentence', self.MASKED_INDICES_COLUMN, 'sentence', self.TARGET_COLUMN,\n",
        "                            self.TOKEN_MASK_COLUMN,\n",
        "                            self.NSP_TARGET_COLUMN]\n",
        "        # 否则，设置self.columns为不包含文本的列名列表\n",
        "        else:\n",
        "            self.columns = [self.MASKED_INDICES_COLUMN, self.TARGET_COLUMN, self.TOKEN_MASK_COLUMN,\n",
        "                            self.NSP_TARGET_COLUMN]\n",
        "\n",
        "        # 调用prepare_dataset方法准备数据集，并将结果赋值给self.df\n",
        "        self.df = self.prepare_dataset()\n",
        "        print(len(self.df))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 根据索引从数据框中获取数据项\n",
        "        item = self.df.iloc[idx]\n",
        "\n",
        "        # 将被遮盖的索引列转换为长整型张量\n",
        "        inp = torch.Tensor(item[self.MASKED_INDICES_COLUMN]).long()\n",
        "        # 将令牌掩码列转换为布尔型张量\n",
        "        token_mask = torch.Tensor(item[self.TOKEN_MASK_COLUMN]).bool()\n",
        "\n",
        "        # 将目标列转换为长整型张量，并用令牌掩码填充为0\n",
        "        mask_target = torch.Tensor(item[self.TARGET_COLUMN]).long()\n",
        "        mask_target = mask_target.masked_fill_(token_mask, 0)\n",
        "\n",
        "        # 生成注意力掩码，判断输入张量中的元素是否等于填充符PAD\n",
        "        attention_mask = (inp == self.vocab[self.PAD]).unsqueeze(0)\n",
        "\n",
        "        # 根据NSP目标列的值，生成NSP目标张量\n",
        "        if item[self.NSP_TARGET_COLUMN] == 0:\n",
        "            t = [1, 0]\n",
        "        else:\n",
        "            t = [0, 1]\n",
        "        nsp_target = torch.Tensor(t)\n",
        "\n",
        "        return (\n",
        "            # 将输入张量、注意力掩码、令牌掩码、目标张量和NSP目标张量转移到指定设备上\n",
        "            inp.to(device),\n",
        "            attention_mask.to(device),\n",
        "            token_mask.to(device),\n",
        "            mask_target.to(device),\n",
        "            nsp_target.to(device)\n",
        "        )\n",
        "\n",
        "    def prepare_dataset(self) -> pd.DataFrame:\n",
        "        # 存储句子的列表\n",
        "        sentences = []\n",
        "        # 存储NSP的列表\n",
        "        nsp = []\n",
        "        # 存储句子长度的列表\n",
        "        sentence_lens = []\n",
        "\n",
        "        # 将数据集按句子进行分割\n",
        "        # Split dataset on sentences\n",
        "        for review in self.ds:\n",
        "            # 将每条评论按'. '分割成句子列表\n",
        "            review_sentences = review.split('. ')\n",
        "            # 将句子列表添加到sentences中\n",
        "            sentences += review_sentences\n",
        "            # 更新句子长度的列表\n",
        "            self._update_length(review_sentences, sentence_lens)\n",
        "        # 找到最优的句子长度\n",
        "        self.optimal_sentence_length = self._find_optimal_sentence_length(sentence_lens)\n",
        "\n",
        "        print(\"找到最优的句子长度:\")\n",
        "        print(self.optimal_sentence_length)\n",
        "\n",
        "        print(\"Create vocabulary\")\n",
        "        # 遍历句子列表\n",
        "        for sentence in tqdm(sentences):\n",
        "            # 对句子进行分词\n",
        "            s = self.tokenizer(sentence)\n",
        "            # 更新词汇计数器\n",
        "            self.counter.update(s)\n",
        "\n",
        "        # 填充词汇表\n",
        "        self._fill_vocab()\n",
        "\n",
        "        print(\"词表词汇数：\")\n",
        "        print(len(self.vocab))\n",
        "\n",
        "        print(\"Preprocessing dataset\")\n",
        "        # 遍历数据集\n",
        "        for review in tqdm(self.ds):\n",
        "            # 将评论按'. '分割成句子列表\n",
        "            review_sentences = review.split('. ')\n",
        "            # 如果句子数量大于1\n",
        "            if len(review_sentences) > 1:\n",
        "                # 遍历句子列表中的每个句子（除了最后一个）\n",
        "                for i in range(len(review_sentences) - 1):\n",
        "                    # 创建一个真正的NSP项\n",
        "                    # True NSP item\n",
        "                    first, second = self.tokenizer(review_sentences[i]), self.tokenizer(review_sentences[i + 1])\n",
        "                    # 将NSP项添加到nsp列表中\n",
        "                    nsp.append(self._create_item(first, second, 1))\n",
        "\n",
        "                    # 创建一个虚假的NSP项\n",
        "                    # False NSP item\n",
        "                    first, second = self._select_false_nsp_sentences(sentences)\n",
        "                    first, second = self.tokenizer(first), self.tokenizer(second)\n",
        "                    # 将NSP项添加到nsp列表中\n",
        "                    nsp.append(self._create_item(first, second, 0))\n",
        "\n",
        "        # 将nsp列表转换为DataFrame\n",
        "        df = pd.DataFrame(nsp, columns=self.columns)\n",
        "        return df\n",
        "\n",
        "    def _update_length(self, sentences: typing.List[str], lengths: typing.List[int]):\n",
        "        for v in sentences:\n",
        "            l = len(v.split())\n",
        "            lengths.append(l)\n",
        "        return lengths\n",
        "\n",
        "    def _find_optimal_sentence_length(self, lengths: typing.List[int]):\n",
        "        # 将长度列表转换为NumPy数组\n",
        "        arr = np.array(lengths)\n",
        "        # 计算数组中指定百分位数的值，并转换为整数返回\n",
        "        return int(np.percentile(arr, self.OPTIMAL_LENGTH_PERCENTILE))\n",
        "\n",
        "    def _fill_vocab(self):\n",
        "        # specials= argument is only in 0.12.0 version\n",
        "        # specials=[self.CLS, self.PAD, self.MASK, self.SEP, self.UNK]\n",
        "        self.vocab = vocab(self.counter, min_freq=2)\n",
        "\n",
        "        # 0.11.0 uses this approach to insert specials\n",
        "        self.vocab.insert_token(self.CLS, 0)\n",
        "        self.vocab.insert_token(self.PAD, 1)\n",
        "        self.vocab.insert_token(self.MASK, 2)\n",
        "        self.vocab.insert_token(self.SEP, 3)\n",
        "        self.vocab.insert_token(self.UNK, 4)\n",
        "        self.vocab.set_default_index(4)\n",
        "\n",
        "    def _create_item(self, first: typing.List[str], second: typing.List[str], target: int = 1):\n",
        "        # 创建带有掩码的句子项\n",
        "        # Create masked sentence item\n",
        "        updated_first, first_mask = self._preprocess_sentence(first.copy())\n",
        "        updated_second, second_mask = self._preprocess_sentence(second.copy())\n",
        "\n",
        "        nsp_sentence = updated_first + [self.SEP] + updated_second\n",
        "        nsp_indices = self.vocab.lookup_indices(nsp_sentence)\n",
        "        inverse_token_mask = first_mask + [True] + second_mask\n",
        "\n",
        "        # 创建未随机掩码单词的句子项\n",
        "        # Create sentence item without masking random words\n",
        "        first, _ = self._preprocess_sentence(first.copy(), should_mask=False)\n",
        "        second, _ = self._preprocess_sentence(second.copy(), should_mask=False)\n",
        "        original_nsp_sentence = first + [self.SEP] + second\n",
        "        original_nsp_indices = self.vocab.lookup_indices(original_nsp_sentence)\n",
        "\n",
        "        if self.should_include_text:\n",
        "            return (\n",
        "                # 带有掩码的句子\n",
        "                nsp_sentence,\n",
        "                # 带有掩码的句子索引\n",
        "                nsp_indices,\n",
        "                # 原始句子\n",
        "                original_nsp_sentence,\n",
        "                # 原始句子索引\n",
        "                original_nsp_indices,\n",
        "                # 逆向标记掩码\n",
        "                inverse_token_mask,\n",
        "                # 目标值\n",
        "                target\n",
        "            )\n",
        "        else:\n",
        "            return (\n",
        "                # 带有掩码的句子索引\n",
        "                nsp_indices,\n",
        "                # 原始句子索引\n",
        "                original_nsp_indices,\n",
        "                # 逆向标记掩码\n",
        "                inverse_token_mask,\n",
        "                # 目标值\n",
        "                target\n",
        "            )\n",
        "\n",
        "    def _select_false_nsp_sentences(self, sentences: typing.List[str]):\n",
        "        \"\"\"Select sentences to create false NSP item\n",
        "\n",
        "        Args:\n",
        "            sentences: list of all sentences\n",
        "\n",
        "        Returns:\n",
        "            tuple of two sentences. The second one NOT the next sentence\n",
        "        \"\"\"\n",
        "        sentences_len = len(sentences)\n",
        "        sentence_index = random.randint(0, sentences_len - 1)\n",
        "        next_sentence_index = random.randint(0, sentences_len - 1)\n",
        "\n",
        "        # 确保它不是真正的下一句\n",
        "        # To be sure that it's not real next sentence\n",
        "        while next_sentence_index == sentence_index + 1:\n",
        "            next_sentence_index = random.randint(0, sentences_len - 1)\n",
        "\n",
        "        return sentences[sentence_index], sentences[next_sentence_index]\n",
        "\n",
        "    def _preprocess_sentence(self, sentence: typing.List[str], should_mask: bool = True):\n",
        "        # 初始化一个空的inverse_token_mask变量\n",
        "        inverse_token_mask = None\n",
        "\n",
        "        # 如果should_mask为True，则对句子进行掩码处理\n",
        "        if should_mask:\n",
        "            # 调用_mask_sentence方法对句子进行掩码处理，并返回处理后的句子和掩码后的inverse_token_mask\n",
        "            sentence, inverse_token_mask = self._mask_sentence(sentence)\n",
        "\n",
        "        # 确保 inverse_token_mask 是一个列表\n",
        "        if inverse_token_mask is None:\n",
        "            inverse_token_mask = []\n",
        "\n",
        "        # 在句子开头添加[CLS]标记，并将inverse_token_mask的开头添加一个True\n",
        "        # 调用_pad_sentence方法对句子进行填充处理，并返回处理后的句子和填充后的inverse_token_mask\n",
        "        sentence, inverse_token_mask = self._pad_sentence([self.CLS] + sentence, [True] + inverse_token_mask)\n",
        "\n",
        "        return sentence, inverse_token_mask\n",
        "\n",
        "    def _mask_sentence(self, sentence: typing.List[str]):\n",
        "        \"\"\"Replace MASK_PERCENTAGE (15%) of words with special [MASK] symbol\n",
        "        or with random word from vocabulary\n",
        "\n",
        "        Args:\n",
        "            sentence: sentence to process\n",
        "\n",
        "        Returns:\n",
        "            tuple of processed sentence and inverse token mask\n",
        "        \"\"\"\n",
        "        len_s = len(sentence)\n",
        "        inverse_token_mask = [True for _ in range(max(len_s, self.optimal_sentence_length))]\n",
        "\n",
        "        # 计算需要遮盖的词的数量\n",
        "        mask_amount = round(len_s * self.MASK_PERCENTAGE)\n",
        "        for _ in range(mask_amount):\n",
        "            # 随机选择一个词的索引\n",
        "            i = random.randint(0, len_s - 1)\n",
        "\n",
        "            # 以0.8的概率用[MASK]符号替换选中的词\n",
        "            if random.random() < 0.8:\n",
        "                sentence[i] = self.MASK\n",
        "            else:\n",
        "                # 以0.2的概率从词汇表中随机选择一个词替换选中的词\n",
        "                # 注意：索引小于5的是特殊标记，参见self._insert_specials方法\n",
        "                # All is below 5 is special token\n",
        "                # see self._insert_specials method\n",
        "                j = random.randint(5, len(self.vocab) - 1)\n",
        "                sentence[i] = self.vocab.lookup_token(j)\n",
        "\n",
        "            # 将遮盖词的索引位置在inverse_token_mask中标记为False\n",
        "            inverse_token_mask[i] = False\n",
        "        return sentence, inverse_token_mask\n",
        "\n",
        "    def _pad_sentence(self, sentence: typing.List[str], inverse_token_mask: typing.List[bool] = None):\n",
        "        len_s = len(sentence)\n",
        "\n",
        "        # 如果句子长度大于等于最优句子长度\n",
        "        if len_s >= self.optimal_sentence_length:\n",
        "            # 取句子前最优句子长度的部分\n",
        "            s = sentence[:self.optimal_sentence_length]\n",
        "        else:\n",
        "            # 否则，在句子末尾补充PAD，直到长度为最优句子长度\n",
        "            s = sentence + [self.PAD] * (self.optimal_sentence_length - len_s)\n",
        "\n",
        "        # 如果提供了inverse_token_mask，也需要进行填充\n",
        "        # inverse token mask should be padded as well\n",
        "        if inverse_token_mask:\n",
        "            len_m = len(inverse_token_mask)\n",
        "\n",
        "            # 如果inverse_token_mask的长度大于等于最优句子长度\n",
        "            if len_m >= self.optimal_sentence_length:\n",
        "                # 取inverse_token_mask前最优句子长度的部分\n",
        "                inverse_token_mask = inverse_token_mask[:self.optimal_sentence_length]\n",
        "            else:\n",
        "                # 否则，在inverse_token_mask末尾补充True，直到长度为最优句子长度\n",
        "                inverse_token_mask = inverse_token_mask + [True] * (self.optimal_sentence_length - len_m)\n",
        "\n",
        "        return s, inverse_token_mask\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5ZAvX3neUkL8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "import torch.nn.functional as f\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class JointEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, size):\n",
        "        # 调用父类的初始化方法\n",
        "        super(JointEmbedding, self).__init__()\n",
        "\n",
        "        # 初始化成员变量 size\n",
        "        self.size = size\n",
        "\n",
        "        # 初始化 token 的嵌入层，大小为 vocab_size x size\n",
        "        self.token_emb = nn.Embedding(vocab_size, size)\n",
        "        # 初始化 segment 的嵌入层，大小为 vocab_size x size\n",
        "        self.segment_emb = nn.Embedding(vocab_size, size)\n",
        "\n",
        "        # 初始化层归一化层，输入特征维度为 size\n",
        "        self.norm = nn.LayerNorm(size)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        # 获取输入张量的句子大小\n",
        "        sentence_size = input_tensor.size(-1)\n",
        "        # 调用 attention_position 方法，传入 self.size 和 input_tensor，得到位置张量\n",
        "        pos_tensor = self.attention_position(self.size, input_tensor)\n",
        "\n",
        "        # 创建一个与 input_tensor 相同形状的零张量，并将其设备设为 device\n",
        "        segment_tensor = torch.zeros_like(input_tensor).to(device)\n",
        "        # 将 segment_tensor 中后半部分的元素设为 1\n",
        "        segment_tensor[:, sentence_size // 2 + 1:] = 1\n",
        "\n",
        "        # 对 input_tensor 进行 token_emb 嵌入，与 segment_tensor 进行 segment_emb 嵌入，并与 pos_tensor 相加，得到输出张量\n",
        "        output = self.token_emb(input_tensor) + self.segment_emb(segment_tensor) + pos_tensor\n",
        "        # 对输出张量进行归一化处理\n",
        "        return self.norm(output)\n",
        "\n",
        "    def attention_position(self, dim, input_tensor):\n",
        "        # 获取输入张量的批处理大小\n",
        "        batch_size = input_tensor.size(0)\n",
        "        # 获取输入张量的句子长度\n",
        "        sentence_size = input_tensor.size(-1)\n",
        "\n",
        "        # 创建一个从0到句子长度的长整型张量，并指定设备\n",
        "        pos = torch.arange(sentence_size, dtype=torch.long).to(device)\n",
        "        # 创建一个从0到维度大小的长整型张量，并指定设备\n",
        "        d = torch.arange(dim, dtype=torch.long).to(device)\n",
        "        # 将维度张量转换为范围在0到2之间的浮点数张量\n",
        "        d = (2 * d / dim)\n",
        "\n",
        "        # 将位置张量增加一个新的维度\n",
        "        pos = pos.unsqueeze(1)\n",
        "        # 根据维度张量对位置张量进行缩放\n",
        "        pos = pos / (1e4 ** d)\n",
        "\n",
        "        # 对位置张量的偶数位置应用正弦函数\n",
        "        pos[:, ::2] = torch.sin(pos[:, ::2])\n",
        "        # 对位置张量的奇数位置应用余弦函数\n",
        "        pos[:, 1::2] = torch.cos(pos[:, 1::2])\n",
        "\n",
        "        # 将位置张量扩展为与输入张量相同大小的张量\n",
        "        return pos.expand(batch_size, *pos.size())\n",
        "\n",
        "    def numeric_position(self, dim, input_tensor):\n",
        "        # 创建一个从0到dim-1的一维张量，数据类型为long，并将其转移到当前设备\n",
        "        pos_tensor = torch.arange(dim, dtype=torch.long).to(device)\n",
        "        # 将pos_tensor扩展为与input_tensor形状相同的张量\n",
        "        return pos_tensor.expand_as(input_tensor)\n",
        "\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_inp, dim_out):\n",
        "        # 调用父类的构造函数\n",
        "        super(AttentionHead, self).__init__()\n",
        "\n",
        "        # 初始化输入维度\n",
        "        self.dim_inp = dim_inp\n",
        "\n",
        "        # 初始化查询向量转换层\n",
        "        self.q = nn.Linear(dim_inp, dim_out)\n",
        "        # 初始化键向量转换层\n",
        "        self.k = nn.Linear(dim_inp, dim_out)\n",
        "        # 初始化值向量转换层\n",
        "        self.v = nn.Linear(dim_inp, dim_out)\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor = None):\n",
        "        # 获取query、key和value\n",
        "        query, key, value = self.q(input_tensor), self.k(input_tensor), self.v(input_tensor)\n",
        "\n",
        "        # 计算缩放因子\n",
        "        scale = query.size(1) ** 0.5\n",
        "        # 计算注意力分数\n",
        "        scores = torch.bmm(query, key.transpose(1, 2)) / scale\n",
        "\n",
        "        # 如果提供了注意力掩码，则使用掩码填充注意力分数\n",
        "        scores = scores.masked_fill_(attention_mask, -1e9)\n",
        "        # 对注意力分数应用softmax函数\n",
        "        attn = f.softmax(scores, dim=-1)\n",
        "        # 计算上下文向量\n",
        "        context = torch.bmm(attn, value)\n",
        "\n",
        "        return context\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, dim_inp, dim_out):\n",
        "        # 调用父类构造函数\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        # 创建一个AttentionHead模块列表，根据num_heads的值初始化每个模块的参数\n",
        "        self.heads = nn.ModuleList([\n",
        "            # 创建一个AttentionHead模块，输入维度为dim_inp，输出维度为dim_out\n",
        "            AttentionHead(dim_inp, dim_out) for _ in range(num_heads)\n",
        "        ])\n",
        "        # 创建一个线性层，输入维度为dim_out * num_heads，输出维度为dim_inp\n",
        "        self.linear = nn.Linear(dim_out * num_heads, dim_inp)\n",
        "        # 创建一个层归一化层，输入维度为dim_inp\n",
        "        self.norm = nn.LayerNorm(dim_inp)\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
        "        # 遍历模型中的每个注意力头，并应用注意力机制\n",
        "        s = [head(input_tensor, attention_mask) for head in self.heads]\n",
        "        # 将所有注意力头的输出在最后一个维度上进行拼接\n",
        "        scores = torch.cat(s, dim=-1)\n",
        "        # 对拼接后的分数进行线性变换\n",
        "        scores = self.linear(scores)\n",
        "        # 对线性变换后的分数进行归一化处理\n",
        "        return self.norm(scores)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_inp, dim_out, attention_heads=4, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # 初始化多头注意力机制模块\n",
        "        # 输入参数为：多头注意力头数、输入维度、输出维度\n",
        "        # 输出维度为：batch_size x sentence size x dim_inp\n",
        "        self.attention = MultiHeadAttention(attention_heads, dim_inp, dim_out)  # batch_size x sentence size x dim_inp\n",
        "\n",
        "        # 初始化前馈神经网络模块\n",
        "        # 包含线性层、dropout层、GELU激活函数、线性层、dropout层\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(dim_inp, dim_out),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim_out, dim_inp),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # 初始化层归一化模块\n",
        "        # 输入参数为：输入维度\n",
        "        self.norm = nn.LayerNorm(dim_inp)\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
        "        # 调用self.attention方法，传入input_tensor和attention_mask作为参数，得到context\n",
        "        context = self.attention(input_tensor, attention_mask)\n",
        "        # 调用self.feed_forward方法，传入context作为参数，得到res\n",
        "        res = self.feed_forward(context)\n",
        "        # 调用self.norm方法，传入res作为参数，得到最终结果并返回\n",
        "        return self.norm(res)\n",
        "\n",
        "\n",
        "class BERT(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, dim_inp, dim_out, attention_heads=4):\n",
        "        super(BERT, self).__init__()\n",
        "\n",
        "        # 初始化联合嵌入层\n",
        "        self.embedding = JointEmbedding(vocab_size, dim_inp)\n",
        "\n",
        "        # 初始化编码器\n",
        "        self.encoder = Encoder(dim_inp, dim_out, attention_heads)\n",
        "\n",
        "        # 初始化单词预测层\n",
        "        # 将输入维度dim_inp映射到词汇表大小vocab_size的线性层\n",
        "        self.token_prediction_layer = nn.Linear(dim_inp, vocab_size)\n",
        "\n",
        "        # 初始化Softmax层\n",
        "        # 对输出进行对数Softmax运算\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "        # 初始化分类层\n",
        "        # 将输入维度dim_inp映射到2个类别的线性层\n",
        "        self.classification_layer = nn.Linear(dim_inp, 2)\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
        "        # 将输入张量进行嵌入操作\n",
        "        embedded = self.embedding(input_tensor)\n",
        "        print(\"embedded shape\")\n",
        "        print(embedded.shape)\n",
        "\n",
        "        # 将嵌入后的张量通过编码器进行编码\n",
        "        encoded = self.encoder(embedded, attention_mask)\n",
        "\n",
        "        # 对编码后的张量进行令牌预测\n",
        "        token_predictions = self.token_prediction_layer(encoded)\n",
        "\n",
        "        # 提取编码后张量的第一个词向量\n",
        "        first_word = encoded[:, 0, :]\n",
        "\n",
        "        # 对令牌预测结果应用softmax操作\n",
        "        # 对第一个词向量进行分类\n",
        "        return self.softmax(token_predictions), self.classification_layer(first_word)\n"
      ],
      "metadata": {
        "id": "Ukb8VhadN65c"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def percentage(batch_size: int, max_index: int, current_index: int):\n",
        "    \"\"\"Calculate epoch progress percentage\n",
        "\n",
        "    Args:\n",
        "        batch_size: batch size\n",
        "        max_index: max index in epoch\n",
        "        current_index: current index\n",
        "\n",
        "    Returns:\n",
        "        Passed percentage of dataset\n",
        "    \"\"\"\n",
        "    batched_max = max_index // batch_size\n",
        "    return round(current_index / batched_max * 100, 2)\n",
        "\n",
        "\n",
        "def nsp_accuracy(result: torch.Tensor, target: torch.Tensor):\n",
        "    \"\"\"Calculate NSP accuracy between two tensors\n",
        "\n",
        "    Args:\n",
        "        result: result calculated by model\n",
        "        target: real target\n",
        "\n",
        "    Returns:\n",
        "        NSP accuracy\n",
        "    \"\"\"\n",
        "    s = (result.argmax(1) == target.argmax(1)).sum()\n",
        "    return round(float(s / result.size(0)), 2)\n",
        "\n",
        "\n",
        "def token_accuracy(result: torch.Tensor, target: torch.Tensor, inverse_token_mask: torch.Tensor):\n",
        "    \"\"\"Calculate MLM accuracy between ONLY masked words\n",
        "\n",
        "    Args:\n",
        "        result: result calculated by model\n",
        "        target: real target\n",
        "        inverse_token_mask: well-known inverse token mask\n",
        "\n",
        "    Returns:\n",
        "        MLM accuracy\n",
        "    \"\"\"\n",
        "    r = result.argmax(-1).masked_select(~inverse_token_mask)\n",
        "    t = target.masked_select(~inverse_token_mask)\n",
        "    s = (r == t).sum()\n",
        "    return round(float(s / (result.size(0) * result.size(1))), 2)\n",
        "\n",
        "\n",
        "class BertTrainer:\n",
        "\n",
        "    def __init__(self,\n",
        "                 model: BERT,\n",
        "                 dataset: IMDBBertDataset,\n",
        "                 log_dir: Path,\n",
        "                 checkpoint_dir: Path = None,\n",
        "                 print_progress_every: int = 10,\n",
        "                 print_accuracy_every: int = 50,\n",
        "                 batch_size: int = 24,\n",
        "                 learning_rate: float = 0.005,\n",
        "                 epochs: int = 5,\n",
        "                 ):\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        self.loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        self.writer = SummaryWriter(str(log_dir))\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "\n",
        "        self.criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "        self.ml_criterion = nn.NLLLoss(ignore_index=0).to(device)\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.015)\n",
        "\n",
        "        self._splitter_size = 35\n",
        "\n",
        "        self._ds_len = len(self.dataset)\n",
        "        self._batched_len = self._ds_len // self.batch_size\n",
        "\n",
        "        self._print_every = print_progress_every\n",
        "        self._accuracy_every = print_accuracy_every\n",
        "\n",
        "    def print_summary(self):\n",
        "        ds_len = len(self.dataset)\n",
        "\n",
        "        print(\"Model Summary\\n\")\n",
        "        print('=' * self._splitter_size)\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"Training dataset len: {ds_len}\")\n",
        "        print(f\"Max / Optimal sentence len: {self.dataset.optimal_sentence_length}\")\n",
        "        print(f\"Vocab size: {len(self.dataset.vocab)}\")\n",
        "        print(f\"Batch size: {self.batch_size}\")\n",
        "        print(f\"Batched dataset len: {self._batched_len}\")\n",
        "        print('=' * self._splitter_size)\n",
        "        print()\n",
        "\n",
        "    def __call__(self):\n",
        "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
        "            loss = self.train(self.current_epoch)\n",
        "            self.save_checkpoint(self.current_epoch, step=-1, loss=loss)\n",
        "\n",
        "    def train(self, epoch: int):\n",
        "        print(f\"Begin epoch {epoch}\")\n",
        "\n",
        "        prev = time.time()\n",
        "        average_nsp_loss = 0\n",
        "        average_mlm_loss = 0\n",
        "        for i, value in enumerate(self.loader):\n",
        "            index = i + 1\n",
        "            inp, mask, inverse_token_mask, token_target, nsp_target = value\n",
        "            self.optimizer.zero_grad()\n",
        "            print(\"inp shape\")\n",
        "            print(inp.shape)\n",
        "            token, nsp = self.model(inp, mask)\n",
        "\n",
        "            tm = inverse_token_mask.unsqueeze(-1).expand_as(token)\n",
        "            token = token.masked_fill(tm, 0)\n",
        "\n",
        "            loss_token = self.ml_criterion(token.transpose(1, 2), token_target)  # 1D tensor as target is required\n",
        "            loss_nsp = self.criterion(nsp, nsp_target)\n",
        "\n",
        "            loss = loss_token + loss_nsp\n",
        "            average_nsp_loss += loss_nsp\n",
        "            average_mlm_loss += loss_token\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if index % self._print_every == 0:\n",
        "                elapsed = time.gmtime(time.time() - prev)\n",
        "                s = self.training_summary(elapsed, index, average_nsp_loss, average_mlm_loss)\n",
        "\n",
        "                if index % self._accuracy_every == 0:\n",
        "                    s += self.accuracy_summary(index, token, nsp, token_target, nsp_target, inverse_token_mask)\n",
        "\n",
        "                print(s)\n",
        "\n",
        "                average_nsp_loss = 0\n",
        "                average_mlm_loss = 0\n",
        "        return loss\n",
        "\n",
        "    def training_summary(self, elapsed, index, average_nsp_loss, average_mlm_loss):\n",
        "        passed = percentage(self.batch_size, self._ds_len, index)\n",
        "        global_step = self.current_epoch * len(self.loader) + index\n",
        "\n",
        "        print_nsp_loss = average_nsp_loss / self._print_every\n",
        "        print_mlm_loss = average_mlm_loss / self._print_every\n",
        "\n",
        "        s = f\"{time.strftime('%H:%M:%S', elapsed)}\"\n",
        "        s += f\" | Epoch {self.current_epoch + 1} | {index} / {self._batched_len} ({passed}%) | \" \\\n",
        "             f\"NSP loss {print_nsp_loss:6.2f} | MLM loss {print_mlm_loss:6.2f}\"\n",
        "\n",
        "        self.writer.add_scalar(\"NSP loss\", print_nsp_loss, global_step=global_step)\n",
        "        self.writer.add_scalar(\"MLM loss\", print_mlm_loss, global_step=global_step)\n",
        "        return s\n",
        "\n",
        "    def accuracy_summary(self, index, token, nsp, token_target, nsp_target, inverse_token_mask):\n",
        "        global_step = self.current_epoch * len(self.loader) + index\n",
        "        nsp_acc = nsp_accuracy(nsp, nsp_target)\n",
        "        token_acc = token_accuracy(token, token_target, inverse_token_mask)\n",
        "\n",
        "        self.writer.add_scalar(\"NSP train accuracy\", nsp_acc, global_step=global_step)\n",
        "        self.writer.add_scalar(\"Token train accuracy\", token_acc, global_step=global_step)\n",
        "\n",
        "        return f\" | NSP accuracy {nsp_acc} | Token accuracy {token_acc}\"\n",
        "\n",
        "    def save_checkpoint(self, epoch, step, loss):\n",
        "        if not self.checkpoint_dir:\n",
        "            return\n",
        "\n",
        "        prev = time.time()\n",
        "        name = f\"bert_epoch{epoch}_step{step}_{datetime.utcnow().timestamp():.0f}.pt\"\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "        }, self.checkpoint_dir.joinpath(name))\n",
        "\n",
        "        print()\n",
        "        print('=' * self._splitter_size)\n",
        "        print(f\"Model saved as '{name}' for {time.time() - prev:.2f}s\")\n",
        "        print('=' * self._splitter_size)\n",
        "        print()\n",
        "\n",
        "    def load_checkpoint(self, path: Path):\n",
        "        print('=' * self._splitter_size)\n",
        "        print(f\"Restoring model {path}\")\n",
        "        checkpoint = torch.load(path)\n",
        "        self.current_epoch = checkpoint['epoch']\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        print(\"Model is restored.\")\n",
        "        print('=' * self._splitter_size)\n"
      ],
      "metadata": {
        "id": "uqe9fZEVPEYq"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "import torch\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "\n",
        "EMB_SIZE = 64\n",
        "HIDDEN_SIZE = 36\n",
        "EPOCHS = 4\n",
        "BATCH_SIZE = 12\n",
        "NUM_HEADS = 4\n",
        "\n",
        "\n",
        "\n",
        "timestamp = datetime.datetime.utcnow().timestamp()\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"Prepare dataset\")\n",
        "    ds = IMDBBertDataset('./data/imdb.csv', ds_from=0, ds_to=1000)\n",
        "\n",
        "    bert = BERT(len(ds.vocab), EMB_SIZE, HIDDEN_SIZE, NUM_HEADS).to(device)\n",
        "    trainer = BertTrainer(\n",
        "        model=bert,\n",
        "        dataset=ds,\n",
        "        log_dir='data/logs/bert_experiment',\n",
        "        checkpoint_dir='./data/bert_checkpoints',\n",
        "        print_progress_every=20,\n",
        "        print_accuracy_every=200,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        learning_rate=0.00007,\n",
        "        epochs=15\n",
        "    )\n",
        "\n",
        "    trainer.print_summary()\n",
        "    trainer()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RXVOLlETPVrS",
        "outputId": "b77ec715-2270-4cc4-adab-e2553a54fa2d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepare dataset\n",
            "找到最优的句子长度:\n",
            "27\n",
            "Create vocabulary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9561/9561 [00:00<00:00, 42218.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "词表词汇数：\n",
            "9626\n",
            "Preprocessing dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 560.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17122\n",
            "Model Summary\n",
            "\n",
            "===================================\n",
            "Device: cpu\n",
            "Training dataset len: 17122\n",
            "Max / Optimal sentence len: 27\n",
            "Vocab size: 9626\n",
            "Batch size: 12\n",
            "Batched dataset len: 1426\n",
            "===================================\n",
            "\n",
            "Begin epoch 0\n",
            "inp shape\n",
            "torch.Size([12, 55])\n",
            "embedded shape\n",
            "torch.Size([12, 55, 64])\n",
            "inp shape\n",
            "torch.Size([12, 55])\n",
            "embedded shape\n",
            "torch.Size([12, 55, 64])\n",
            "inp shape\n",
            "torch.Size([12, 55])\n",
            "embedded shape\n",
            "torch.Size([12, 55, 64])\n",
            "inp shape\n",
            "torch.Size([12, 55])\n",
            "embedded shape\n",
            "torch.Size([12, 55, 64])\n",
            "inp shape\n",
            "torch.Size([12, 55])\n",
            "embedded shape\n",
            "torch.Size([12, 55, 64])\n",
            "inp shape\n",
            "torch.Size([12, 55])\n",
            "embedded shape\n",
            "torch.Size([12, 55, 64])\n",
            "inp shape\n",
            "torch.Size([12, 55])\n",
            "embedded shape\n",
            "torch.Size([12, 55, 64])\n",
            "inp shape\n",
            "torch.Size([12, 55])\n",
            "embedded shape\n",
            "torch.Size([12, 55, 64])\n",
            "inp shape\n",
            "torch.Size([12, 55])\n",
            "embedded shape\n",
            "torch.Size([12, 55, 64])\n",
            "inp shape\n",
            "torch.Size([12, 55])\n",
            "embedded shape\n",
            "torch.Size([12, 55, 64])\n",
            "inp shape\n",
            "torch.Size([12, 55])\n",
            "embedded shape\n",
            "torch.Size([12, 55, 64])\n",
            "inp shape\n",
            "torch.Size([12, 55])\n",
            "embedded shape\n",
            "torch.Size([12, 55, 64])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-ae6addb6f2aa>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-e9a96e0f3f85>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-e9a96e0f3f85>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0maverage_mlm_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}